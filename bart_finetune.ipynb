{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Clone the github repository with the finetuning script and install requirements\n",
        "!git clone https://github.com/huggingface/transformers\n",
        "%cd transformers\n",
        "!pip install .\n",
        "%cd examples/pytorch/summarization\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u68Rtaj6MFwn",
        "outputId": "2c82954b-5aa8-4f19-c94c-b093507db8bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 204128, done.\u001b[K\n",
            "remote: Counting objects: 100% (1953/1953), done.\u001b[K\n",
            "remote: Compressing objects: 100% (746/746), done.\u001b[K\n",
            "remote: Total 204128 (delta 1278), reused 1613 (delta 1086), pack-reused 202175\u001b[K\n",
            "Receiving objects: 100% (204128/204128), 214.86 MiB | 12.77 MiB/s, done.\n",
            "Resolving deltas: 100% (146089/146089), done.\n",
            "/content/transformers\n",
            "Processing /content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.41.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.41.0.dev0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.0.dev0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.0.dev0) (2024.2.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.41.0.dev0-py3-none-any.whl size=9045382 sha256=1dfe4b67f1ba59d99542c9a002112abc46a459ec2288b1786e5cacf50b2b9dbe\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xkjv3taj/wheels/7c/35/80/e946b22a081210c6642e607ed65b2a5b9a4d9259695ee2caf5\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.40.1\n",
            "    Uninstalling transformers-4.40.1:\n",
            "      Successfully uninstalled transformers-4.40.1\n",
            "Successfully installed transformers-4.41.0.dev0\n",
            "/content/transformers/examples/pytorch/summarization\n",
            "Collecting accelerate>=0.12.0 (from -r requirements.txt (line 1))\n",
            "  Downloading accelerate-0.30.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=1.8.0 (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.20.3)\n",
            "Collecting rouge-score (from -r requirements.txt (line 5))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.8.1)\n",
            "Collecting py7zr (from -r requirements.txt (line 7))\n",
            "  Downloading py7zr-0.21.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.2.1+cu121)\n",
            "Collecting evaluate (from -r requirements.txt (line 9))\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.66.2)\n",
            "Collecting xxhash (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.9.5)\n",
            "Collecting huggingface-hub (from accelerate>=0.12.0->-r requirements.txt (line 1))\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 6)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 6)) (2023.12.25)\n",
            "Collecting texttable (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pycryptodomex>=3.16.0 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzstd>=0.15.9 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading pyzstd-0.15.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (411 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.2/411.2 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyppmd<1.2.0,>=1.1.0 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybcj<1.1.0,>=1.0.0 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli>=1.1.0 (from py7zr->-r requirements.txt (line 7))\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.3->-r requirements.txt (line 8))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.3->-r requirements.txt (line 8))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.3->-r requirements.txt (line 8))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.3->-r requirements.txt (line 8))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.3->-r requirements.txt (line 8))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.3->-r requirements.txt (line 8))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.3->-r requirements.txt (line 8))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.3->-r requirements.txt (line 8))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.3->-r requirements.txt (line 8))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.3->-r requirements.txt (line 8))\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.3->-r requirements.txt (line 8))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3->-r requirements.txt (line 8))\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->-r requirements.txt (line 8)) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->-r requirements.txt (line 8)) (1.3.0)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=6bbd248e85727ea8e9e8c49d290bd35778288cc35a9c0703b687f8ed98b66ef4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: texttable, brotli, xxhash, pyzstd, pyppmd, pycryptodomex, pybcj, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multivolumefile, inflate64, dill, rouge-score, py7zr, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, nvidia-cusolver-cu12, datasets, evaluate, accelerate\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "from google.colab import drive\n",
        "import json"
      ],
      "metadata": {
        "id": "bYFn-ntd5OPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA (GPU support) is available\n",
        "if torch.cuda.is_available():\n",
        "    # Use CUDA (GPU)\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    # Use CPU\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('Using CPU.')"
      ],
      "metadata": {
        "id": "NQJakS2wFf3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am0Rc20H_7Ng"
      },
      "outputs": [],
      "source": [
        "# Mount drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Make sure all the correct files are in the drive\n",
        "%cd /content/drive/MyDrive/cpsc_477\n",
        "# Add run_summarization2.py to the content\n",
        "# run_summarization.py is a script to finetune these kinds of abstraction models,\n",
        "# but our dataset's format did not work, so I copied it to a file called\n",
        "# run_summarization2.py and edited it to work for our datasets\n",
        "!cp /content/drive/MyDrive/cpsc_477/run_summarization2.py /content/\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run script to finetune our model and save checkpoints\n",
        "\n",
        "!python run_summarization2.py \\  # execute summarization script\n",
        "    --model_name_or_path facebook/bart-base \\\n",
        "    --do_train \\  # trains model\n",
        "    --do_eval \\  # evaluates model\n",
        "    --train_file \"/content/drive/MyDrive/cpsc_477/data/copy_PLOS_train.jsonl\" \\\n",
        "    --validation_file \"/content/drive/MyDrive/cpsc_477/data/copy_PLOS_val.jsonl\" \\\n",
        "    --text_column article \\  # specifies the name of column containing the article text\n",
        "    --summary_column lay_summary \\  # specifies name of the column containing the target summary text\n",
        "    --output_dir \"/content/drive/MyDrive/cpsc_477/training_2\" \\  # directory where model checkpoints and outputs will be saved\n",
        "    --overwrite_output_dir \\  # overwrites the contents of output directory if exists\n",
        "    --per_device_train_batch_size=4 \\  # training batch size for each device\n",
        "    --per_device_eval_batch_size=4 \\  # evaluating batch size for each device\n",
        "    --num_train_epochs=3 \\\n",
        "    --predict_with_generate \\  # specifies whether to use generation during prediction\n",
        "    --save_steps=500 \\  # saves a checkpoint every specified number of steps\n",
        "    --logging_first_step=True \\  # logs metrics and evaluation results after first training step\n",
        "    --logging_steps=500 \\  # logs metrics and evaluation results every specified number of steps\n",
        "    --eval_steps=500  # evaluates model every specified number of steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiGMGU50AKj1",
        "outputId": "540f68fe-5dcb-4183-fc02-5262974a2f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-24 18:23:51.476171: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-24 18:23:51.476217: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-24 18:23:51.477661: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-24 18:23:51.485470: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-24 18:23:52.642488: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/24/2024 18:23:58 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/24/2024 18:23:58 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_steps=500.0,\n",
            "eval_strategy=no,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/drive/MyDrive/cpsc_477/training_2/runs/Apr24_18-23-58_cd1b65627130,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=/content/drive/MyDrive/cpsc_477/training_2,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/drive/MyDrive/cpsc_477/training_2,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-81423fb0290bd9db\n",
            "04/24/2024 18:23:58 - INFO - datasets.builder - Using custom data configuration default-81423fb0290bd9db\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "04/24/2024 18:23:58 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-81423fb0290bd9db/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)\n",
            "04/24/2024 18:23:58 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-81423fb0290bd9db/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-81423fb0290bd9db/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7...\n",
            "04/24/2024 18:23:58 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-81423fb0290bd9db/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7...\n",
            "Downloading took 0.0 min\n",
            "04/24/2024 18:23:58 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "04/24/2024 18:23:58 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "04/24/2024 18:24:00 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 24773 examples [00:18, 1330.93 examples/s]\n",
            "Generating validation split\n",
            "04/24/2024 18:24:18 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 1376 examples [00:00, 1444.77 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "04/24/2024 18:24:19 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-81423fb0290bd9db/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7. Subsequent calls will reuse this data.\n",
            "04/24/2024 18:24:19 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-81423fb0290bd9db/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7. Subsequent calls will reuse this data.\n",
            "config.json: 100% 1.72k/1.72k [00:00<00:00, 11.6MB/s]\n",
            "[INFO|configuration_utils.py:726] 2024-04-24 18:24:20,132 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-24 18:24:20,139 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:655] 2024-04-24 18:24:20,190 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:726] 2024-04-24 18:24:20,235 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-24 18:24:20,236 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 14.4MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 5.23MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 36.4MB/s]\n",
            "[INFO|tokenization_utils_base.py:2088] 2024-04-24 18:24:20,885 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2088] 2024-04-24 18:24:20,886 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2088] 2024-04-24 18:24:20,886 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2088] 2024-04-24 18:24:20,886 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2088] 2024-04-24 18:24:20,886 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2088] 2024-04-24 18:24:20,886 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:726] 2024-04-24 18:24:20,886 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-24 18:24:20,887 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "model.safetensors: 100% 558M/558M [00:02<00:00, 196MB/s]\n",
            "[INFO|modeling_utils.py:3431] 2024-04-24 18:24:24,055 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/model.safetensors\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 18:24:24,128 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4172] 2024-04-24 18:24:25,432 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4180] 2024-04-24 18:24:25,432 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "[INFO|modeling_utils.py:3721] 2024-04-24 18:24:25,484 >> Generation config file not found, using a generation config created from the model config.\n",
            "Running tokenizer on train dataset:   0% 0/24773 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-81423fb0290bd9db/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-caef7631a88d097d.arrow\n",
            "04/24/2024 18:24:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-81423fb0290bd9db/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-caef7631a88d097d.arrow\n",
            "Running tokenizer on train dataset: 100% 24773/24773 [09:36<00:00, 42.96 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/1376 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-81423fb0290bd9db/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a264bb1acd0c738e.arrow\n",
            "04/24/2024 18:34:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-81423fb0290bd9db/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-a264bb1acd0c738e.arrow\n",
            "Running tokenizer on validation dataset: 100% 1376/1376 [00:31<00:00, 43.01 examples/s]\n",
            "Downloading builder script: 100% 6.27k/6.27k [00:00<00:00, 17.9MB/s]\n",
            "[INFO|trainer.py:2054] 2024-04-24 18:34:36,009 >> ***** Running training *****\n",
            "[INFO|trainer.py:2055] 2024-04-24 18:34:36,009 >>   Num examples = 24,773\n",
            "[INFO|trainer.py:2056] 2024-04-24 18:34:36,010 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2057] 2024-04-24 18:34:36,010 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:2060] 2024-04-24 18:34:36,010 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:2061] 2024-04-24 18:34:36,010 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2062] 2024-04-24 18:34:36,010 >>   Total optimization steps = 18,582\n",
            "[INFO|trainer.py:2063] 2024-04-24 18:34:36,011 >>   Number of trainable parameters = 139,420,416\n",
            "{'loss': 4.4299, 'grad_norm': 21.927675247192383, 'learning_rate': 4.99973092239802e-05, 'epoch': 0.0}\n",
            "{'loss': 2.9417, 'grad_norm': 5.783890247344971, 'learning_rate': 4.865461199009795e-05, 'epoch': 0.08}\n",
            "  3% 500/18582 [05:15<3:13:19,  1.56it/s][INFO|trainer.py:3313] 2024-04-24 18:39:51,601 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-500\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 18:39:51,602 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 18:39:51,609 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 18:39:51,610 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 18:39:51,617 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 18:39:54,686 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 18:39:54,695 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 18:39:54,703 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 2.7676, 'grad_norm': 5.11018180847168, 'learning_rate': 4.7309223980195885e-05, 'epoch': 0.16}\n",
            "  5% 1000/18582 [10:46<3:08:11,  1.56it/s][INFO|trainer.py:3313] 2024-04-24 18:45:22,787 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-1000\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 18:45:22,788 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 18:45:22,794 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 18:45:22,796 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 18:45:22,801 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 18:45:29,281 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-1000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 18:45:29,289 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 18:45:29,295 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 2.738, 'grad_norm': 5.340190410614014, 'learning_rate': 4.596383597029384e-05, 'epoch': 0.24}\n",
            "  8% 1500/18582 [16:20<3:03:01,  1.56it/s][INFO|trainer.py:3313] 2024-04-24 18:50:56,103 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-1500\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 18:50:56,105 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 18:50:56,119 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-1500/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 18:50:56,120 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 18:50:56,127 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-1500/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 18:50:58,783 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-1500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 18:50:58,790 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 18:50:58,795 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 2.6633, 'grad_norm': 4.865962505340576, 'learning_rate': 4.461844796039178e-05, 'epoch': 0.32}\n",
            " 11% 2000/18582 [21:53<2:59:01,  1.54it/s][INFO|trainer.py:3313] 2024-04-24 18:56:29,786 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-2000\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 18:56:29,788 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 18:56:29,797 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-2000/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 18:56:29,799 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 18:56:29,806 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-2000/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 18:56:33,017 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-2000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 18:56:33,024 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 18:56:33,034 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 2.6261, 'grad_norm': 5.364024639129639, 'learning_rate': 4.3273059950489726e-05, 'epoch': 0.4}\n",
            " 13% 2500/18582 [27:27<2:52:19,  1.56it/s][INFO|trainer.py:3313] 2024-04-24 19:02:03,490 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-2500\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 19:02:03,492 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 19:02:03,500 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-2500/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 19:02:03,501 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 19:02:03,506 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-2500/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 19:02:05,808 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-2500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 19:02:05,814 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 19:02:05,823 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 2.6051, 'grad_norm': 4.600183010101318, 'learning_rate': 4.1927671940587664e-05, 'epoch': 0.48}\n",
            " 16% 3000/18582 [33:01<2:46:50,  1.56it/s][INFO|trainer.py:3313] 2024-04-24 19:07:37,392 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-3000\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 19:07:37,394 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 19:07:37,400 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-3000/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 19:07:37,401 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 19:07:37,408 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-3000/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 19:07:44,270 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-3000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 19:07:44,276 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 19:07:44,281 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 2.5758, 'grad_norm': 4.758690357208252, 'learning_rate': 4.058228393068561e-05, 'epoch': 0.57}\n",
            " 19% 3500/18582 [38:42<2:41:06,  1.56it/s][INFO|trainer.py:3313] 2024-04-24 19:13:18,687 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-3500\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 19:13:18,689 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 19:13:18,695 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-3500/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 19:13:18,696 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 19:13:18,703 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-3500/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 19:13:21,061 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-3500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 19:13:21,068 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 19:13:21,075 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 2.569, 'grad_norm': 4.911471843719482, 'learning_rate': 3.9236895920783554e-05, 'epoch': 0.65}\n",
            " 22% 4000/18582 [44:14<2:36:54,  1.55it/s][INFO|trainer.py:3313] 2024-04-24 19:18:50,095 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-4000\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 19:18:50,097 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 19:18:50,102 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-4000/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 19:18:50,103 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 19:18:50,110 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-4000/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 19:18:52,499 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-4000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 19:18:52,505 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 19:18:52,512 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 2.5349, 'grad_norm': 4.319770812988281, 'learning_rate': 3.78915079108815e-05, 'epoch': 0.73}\n",
            " 24% 4500/18582 [49:50<2:31:18,  1.55it/s][INFO|trainer.py:3313] 2024-04-24 19:24:26,213 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-4500\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 19:24:26,214 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 19:24:26,223 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-4500/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 19:24:26,224 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 19:24:26,230 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-4500/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 19:24:28,558 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-4500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 19:24:28,564 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 19:24:28,570 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-4500/special_tokens_map.json\n",
            "{'loss': 2.5286, 'grad_norm': 4.540975570678711, 'learning_rate': 3.654611990097944e-05, 'epoch': 0.81}\n",
            " 27% 5000/18582 [55:20<2:26:27,  1.55it/s][INFO|trainer.py:3313] 2024-04-24 19:29:56,166 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-5000\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 19:29:56,168 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 19:29:56,175 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-5000/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 19:29:56,176 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 19:29:56,183 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-5000/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 19:30:01,195 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-5000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 19:30:01,201 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 19:30:01,207 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-5000/special_tokens_map.json\n",
            "{'loss': 2.517, 'grad_norm': 5.015864372253418, 'learning_rate': 3.520073189107739e-05, 'epoch': 0.89}\n",
            " 30% 5500/18582 [1:00:53<2:20:58,  1.55it/s][INFO|trainer.py:3313] 2024-04-24 19:35:29,366 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-5500\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 19:35:29,368 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 19:35:29,375 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-5500/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 19:35:29,376 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 19:35:29,384 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-5500/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 19:35:31,869 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-5500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 19:35:31,876 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-5500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 19:35:31,881 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-5500/special_tokens_map.json\n",
            "{'loss': 2.4995, 'grad_norm': 5.1725592613220215, 'learning_rate': 3.385534388117533e-05, 'epoch': 0.97}\n",
            " 32% 6000/18582 [1:06:23<2:14:43,  1.56it/s][INFO|trainer.py:3313] 2024-04-24 19:40:59,303 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-6000\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 19:40:59,305 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 19:40:59,311 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-6000/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 19:40:59,312 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 19:40:59,319 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-6000/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 19:41:05,502 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-6000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 19:41:05,509 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-6000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 19:41:05,515 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-6000/special_tokens_map.json\n",
            "{'loss': 2.395, 'grad_norm': 5.464829444885254, 'learning_rate': 3.250995587127328e-05, 'epoch': 1.05}\n",
            " 35% 6500/18582 [1:11:55<2:08:34,  1.57it/s][INFO|trainer.py:3313] 2024-04-24 19:46:31,748 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-6500\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 19:46:31,749 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 19:46:31,757 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-6500/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 19:46:31,757 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 19:46:31,764 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-6500/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 19:46:38,282 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-6500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 19:46:38,289 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-6500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 19:46:38,296 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-6500/special_tokens_map.json\n",
            "{'loss': 2.3358, 'grad_norm': 4.696784496307373, 'learning_rate': 3.1164567861371216e-05, 'epoch': 1.13}\n",
            " 38% 7000/18582 [1:17:30<2:03:20,  1.56it/s][INFO|trainer.py:3313] 2024-04-24 19:52:06,112 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-7000\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 19:52:06,114 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 19:52:06,120 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-7000/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 19:52:06,121 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 19:52:06,126 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-7000/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 19:52:13,076 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-7000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 19:52:13,097 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-7000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 19:52:13,104 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-7000/special_tokens_map.json\n",
            "{'loss': 2.311, 'grad_norm': 4.457993984222412, 'learning_rate': 2.9819179851469164e-05, 'epoch': 1.21}\n",
            " 40% 7500/18582 [1:23:04<1:58:38,  1.56it/s][INFO|trainer.py:3313] 2024-04-24 19:57:40,365 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-7500\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 19:57:40,366 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 19:57:40,372 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-7500/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 19:57:40,373 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 19:57:40,379 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-7500/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 19:57:45,456 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-7500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 19:57:45,952 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-7500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 19:57:45,960 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-7500/special_tokens_map.json\n",
            "{'loss': 2.3185, 'grad_norm': 4.502714157104492, 'learning_rate': 2.847379184156711e-05, 'epoch': 1.29}\n",
            " 43% 8000/18582 [1:28:43<1:53:05,  1.56it/s][INFO|trainer.py:3313] 2024-04-24 20:03:19,149 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-8000\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 20:03:19,151 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 20:03:19,157 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-8000/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 20:03:19,157 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 20:03:19,175 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-8000/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 20:03:21,614 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-8000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 20:03:21,621 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-8000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 20:03:21,626 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-8000/special_tokens_map.json\n",
            "{'loss': 2.297, 'grad_norm': 4.6158576011657715, 'learning_rate': 2.7128403831665057e-05, 'epoch': 1.37}\n",
            " 46% 8500/18582 [1:34:12<1:48:42,  1.55it/s][INFO|trainer.py:3313] 2024-04-24 20:08:48,153 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-8500\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 20:08:48,155 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 20:08:48,164 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-8500/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 20:08:48,165 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 20:08:48,172 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-8500/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 20:08:54,552 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-8500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 20:08:54,559 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-8500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 20:08:54,566 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-8500/special_tokens_map.json\n",
            "{'loss': 2.315, 'grad_norm': 4.286260604858398, 'learning_rate': 2.5783015821762995e-05, 'epoch': 1.45}\n",
            " 48% 9000/18582 [1:39:46<1:43:25,  1.54it/s][INFO|trainer.py:3313] 2024-04-24 20:14:22,064 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-9000\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 20:14:22,066 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 20:14:22,072 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-9000/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 20:14:22,073 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 20:14:22,079 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-9000/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 20:14:24,657 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-9000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 20:14:24,664 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-9000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 20:14:24,670 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-9000/special_tokens_map.json\n",
            "{'loss': 2.3163, 'grad_norm': 4.462878227233887, 'learning_rate': 2.4437627811860943e-05, 'epoch': 1.53}\n",
            " 51% 9500/18582 [1:45:15<1:37:55,  1.55it/s][INFO|trainer.py:3313] 2024-04-24 20:19:51,425 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-9500\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 20:19:51,427 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 20:19:51,434 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-9500/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 20:19:51,435 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 20:19:51,441 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-9500/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 20:19:59,837 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-9500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 20:19:59,843 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-9500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 20:19:59,849 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-9500/special_tokens_map.json\n",
            "{'loss': 2.3008, 'grad_norm': 3.988513946533203, 'learning_rate': 2.3092239801958887e-05, 'epoch': 1.61}\n",
            " 54% 10000/18582 [1:50:53<1:32:01,  1.55it/s][INFO|trainer.py:3313] 2024-04-24 20:25:29,089 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-10000\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 20:25:29,091 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 20:25:29,097 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-10000/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 20:25:29,098 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 20:25:29,120 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-10000/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 20:25:35,099 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-10000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 20:25:35,109 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-10000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 20:25:35,115 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-10000/special_tokens_map.json\n",
            "{'loss': 2.306, 'grad_norm': 4.6048407554626465, 'learning_rate': 2.174685179205683e-05, 'epoch': 1.7}\n",
            " 57% 10500/18582 [1:56:31<1:25:58,  1.57it/s][INFO|trainer.py:3313] 2024-04-24 20:31:07,478 >> Saving model checkpoint to /content/drive/MyDrive/cpsc_477/training_2/checkpoint-10500\n",
            "[WARNING|configuration_utils.py:447] 2024-04-24 20:31:07,479 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-04-24 20:31:07,485 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-10500/config.json\n",
            "[INFO|configuration_utils.py:936] 2024-04-24 20:31:07,486 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:705] 2024-04-24 20:31:07,492 >> Configuration saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-10500/generation_config.json\n",
            "[INFO|modeling_utils.py:2592] 2024-04-24 20:31:15,689 >> Model weights saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-10500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2024-04-24 20:31:15,696 >> tokenizer config file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-10500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2024-04-24 20:31:15,701 >> Special tokens file saved in /content/drive/MyDrive/cpsc_477/training_2/checkpoint-10500/special_tokens_map.json\n",
            " 57% 10566/18582 [1:57:28<1:27:49,  1.52it/s]Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/cpsc_477/run_summarization2.py\", line 791, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/cpsc_477/run_summarization2.py\", line 710, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1865, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2209, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3144, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3167, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\", line 1728, in forward\n",
            "    outputs = self.model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\", line 1614, in forward\n",
            "    decoder_outputs = self.decoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\", line 1400, in forward\n",
            "    encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py\", line 449, in _prepare_4d_attention_mask_for_sdpa\n",
            "    if is_tracing:\n",
            "KeyboardInterrupt\n",
            " 57% 10566/18582 [1:57:28<1:29:07,  1.50it/s]\n"
          ]
        }
      ]
    }
  ]
}