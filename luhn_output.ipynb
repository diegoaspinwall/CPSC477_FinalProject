{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Download necessary nltk word sets\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "YcPkeDoSsUFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import re\n",
        "import json\n",
        "from google.colab import drive\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize"
      ],
      "metadata": {
        "id": "j1AALvrcnBil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vneWzAFHoOaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean our article\n",
        "def clean_article(article):\n",
        "\tlem = WordNetLemmatizer()\n",
        "\tarticle =  re.sub(r'\\[[^\\]]*\\]','',article) # remove citations\n",
        "\tarticle = sent_tokenize(article) # tokenize article into sentences\n",
        "\tcleaned_list=[]\n",
        "\tfor sent in article:\n",
        "\t\tsent  = sent.lower()\n",
        "\t\tword_list = []\n",
        "\t\twords = word_tokenize(sent) # tokenize sntences into words\n",
        "\t\tfor word in words:\n",
        "\t\t\tword_list.append(lem.lemmatize(word.lower())) # lemmatize words\n",
        "\t\tcleaned_list.append(' '.join(word_list)) # join lemmatized words\n",
        "\treturn cleaned_list"
      ],
      "metadata": {
        "id": "SLLPOWAFnRqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate word frequencies dict from content\n",
        "def init_freq_dict(content):\n",
        "\tfrequency = {}\n",
        "\tfor sentence in content:\n",
        "\t\tword_list = word_tokenize(sentence) # tokenize sentence into words\n",
        "\t\tfor word in word_list:\n",
        "      # Update frequency of the word in the frequency dictionary as long as not stop word or special character\n",
        "\t\t\tif word not in set(stopwords.words('english')).union({',','.',';','%',')','(','``'}):\n",
        "\t\t\t\tif frequency.get(word) is None:\n",
        "\t\t\t\t\tfrequency[word] = 1 # Initialize if not in dict already\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tfrequency[word] += 1 # Increment by 1 if in dict\n",
        "\treturn frequency"
      ],
      "metadata": {
        "id": "q5oMqnT6nT3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate scores for each sentence based on word frequency and position\n",
        "def get_score(content, frequency_dictionary):\n",
        "    sentence_score = {}\n",
        "    for sentence in content:\n",
        "        score = 0\n",
        "        word_list = word_tokenize(sentence) # tokenize sentence into words\n",
        "        start_idx, end_idx = -1, len(word_list) + 1\n",
        "        index_list = []\n",
        "        for word in word_list:\n",
        "            # Make sure word is not stop word or punctuation and it exists in dict\n",
        "            if word not in set(stopwords.words('english')).union({',', '.', ';', '%', ')', '(', '``'}) and word in frequency_dictionary.keys():\n",
        "                # Add if it passes\n",
        "                index_list.append(word_list.index(word))\n",
        "        # Make sure there are relevant words\n",
        "        if index_list:\n",
        "            # Calculate score based on the number of relevant words and their positions\n",
        "            if max(index_list) - min(index_list) != 0:  # Make sure denominator is non-zero\n",
        "                score = len(index_list) ** 2 / (max(index_list) - min(index_list))\n",
        "            else:\n",
        "                score = len(index_list) ** 2  # Assign high score if denominator is zero\n",
        "        # Put calculated sentence score in dict, with sentence as key\n",
        "        sentence_score[content.index(sentence)] = score\n",
        "    return sentence_score"
      ],
      "metadata": {
        "id": "6IMmifLwnXz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate summary based on sentence scores\n",
        "def summarize(sentence_scores,content,threshold):\n",
        "\tsummary = \"\"\n",
        "  # Sort sentence scores by values in descending order, take top threshold - 1 sentences\n",
        "\tsentence_indices = sorted(sentence_scores,key=sentence_scores.get,reverse=True)[:threshold-1]\n",
        "  # Iterate over indices to extract sentences form content\n",
        "\tfor index in sentence_indices:\n",
        "\t\tsummary+=content[index]+\" \"\n",
        "\treturn summary"
      ],
      "metadata": {
        "id": "Xmcq3IrTnZ9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract articles from jsonl file so generate summaries from\n",
        "def get_articles():\n",
        "\t# Extract summaries from our validation set\n",
        "\tjsonl_file_path = \"/content/drive/MyDrive/cpsc_477/data/copy_PLOS_val.jsonl\"\n",
        "\t# Read the jsonl file\n",
        "\twith open(jsonl_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "\t\t\tarticles = []\n",
        "\t\t\tfor line in f:\n",
        "\t\t\t\t\t# Load the jsonl object on the line\n",
        "\t\t\t\t\tarticle_object = json.loads(line)\n",
        "\t\t\t\t\t# Just extract the article to place in a list\n",
        "\t\t\t\t\tarticle_text = article_object['article']\n",
        "\t\t\t\t\tarticles.append(article_text)\n",
        "\treturn articles"
      ],
      "metadata": {
        "id": "XRuFrABrncg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write summaries to a text file for inference\n",
        "def write_summaries_to_file(summaries, output_file_path):\n",
        "    \"\"\"\n",
        "    Write summaries to a text file.\n",
        "\n",
        "    Args:\n",
        "    - summaries (list): List of summaries.\n",
        "    - output_file_path (str): Path to the output text file.\n",
        "    \"\"\"\n",
        "    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for summary in summaries:\n",
        "            f.write(summary.strip() + \"\\n\")\n",
        "\n",
        "# Count lines in file to make sure it is the right amount for inference\n",
        "def count_lines_in_file(file_path):\n",
        "    \"\"\"\n",
        "    Count the number of lines in a text file.\n",
        "\n",
        "    Args:\n",
        "    - file_path (str): Path to the text file.\n",
        "\n",
        "    Returns:\n",
        "    - int: Number of lines in the file.\n",
        "    \"\"\"\n",
        "    line_count = 0\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line_count += 1\n",
        "    return line_count"
      ],
      "metadata": {
        "id": "hkRHU-a6nfNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function for summarization\n",
        "def run_summarization():\n",
        "\tarticles = get_articles() # extract articles for summarization\n",
        "\tsummaries = []\n",
        "\tfor content in articles:\n",
        "\t\tcleaned_content = clean_article(content) # clean articles\n",
        "\t\tthreshold = len(cleaned_content)//60 # set threshold to pass to summarize\n",
        "\t\tfrequency_dictionary = init_freq_dict(cleaned_content) # initialize our freq dict\n",
        "\t\tsorted_dictionary = {key: frequency_dictionary[key] for key in sorted(frequency_dictionary,key=frequency_dictionary.get,reverse=True)[:300]} # sort dict by freq\n",
        "\t\tsentence_scores = get_score(cleaned_content,sorted_dictionary) # calculate sentence scores based on freq\n",
        "\t\tsummary = summarize(sentence_scores,sent_tokenize(content),threshold).strip() # calculate summary and strip\n",
        "\t\t# Remove all newline characters from summary since we do not want to go to\n",
        "\t\t# a new line in the middle of the summary\n",
        "\t\tsummary = summary.replace(\"\\n\", \"\")\n",
        "\t\t# Check if summary is blank after removing newline characters, if so it failed on that summary\n",
        "\t\tif not summary.strip():\n",
        "\t\t\t\tsummary = \"None.\"\n",
        "\t\tsummaries.append(summary.strip())\n",
        "\n",
        "\t# Write summaries to text file for inference, one per line\n",
        "\twrite_summaries_to_file(summaries, \"/content/drive/MyDrive/cpsc_477/data/luhn_summaries3.txt\")\n",
        "  # Count number of lines in file just generated to make sure correct\n",
        "\tnum_lines = count_lines_in_file(\"/content/drive/MyDrive/cpsc_477/data/luhn_summaries3.txt\")\n",
        "  # Can print for debugging\n",
        "\t# print(\"Number of lines in the file:\", num_lines)"
      ],
      "metadata": {
        "id": "dJmML56wuM_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call function to generate summaries and write to file\n",
        "run_summarization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFS4g1dDfXZZ",
        "outputId": "194ba668-d596-4b52-b900-86053fff828f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "dict_keys(['lay_summary', 'article', 'headings', 'keywords', 'id'])\n",
            "1376\n",
            "1376\n",
            "Number of lines in the file: 1376\n"
          ]
        }
      ]
    }
  ]
}